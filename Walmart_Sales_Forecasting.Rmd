---
title: "Walmart_Time_Series_Analysis"
output: pdf_document
date: "2022-11-15"
---

```{r}
# import dependencies
library(moments)
library(tidyr)
library(dplyr)
library(corrplot)
library(ggplot2)
library(tidyverse)
library(tidyr)
library(lubridate)
library(forecast)
library(tseries)
library(zoo)

```


# Importing the dataset
```{r}

walmart_df <- read.csv("Dataset/walmart-sales-dataset-of-45stores.csv")
walmart_df


#walmart_df$Date <- as.Date(walmart_df$Date, format = "%d-%m-%Y")

# Seperating the Date
#walmart_df[,"Year"] <- format(walmart_df[,"Date"],"%Y")
#walmart_df[,"Month"] <- format(walmart_df[,"Date"],"%m")
#walmart_df[,"Day"] <- format(walmart_df[,"Date"],"%d")

# Dataset dimension
dim(walmart_df)

#The dataset contains 6435 rows and 11 columns

#Checking null vales
colSums(is.na(walmart_df))

#The dataset contains no missing values


summary(walmart_df)

```

# Visualization

```{r}

# Let's find out the distribution of the variables.

par(mfrow=c(3,2))
hist(walmart_df$Temperature, col = 'light blue', main = "Temperature")
hist(walmart_df$Fuel_Price, col = 'light blue', main = "Fuel Price")
hist(walmart_df$CPI, col = 'light blue', main = "CPI")
hist(walmart_df$Unemployment, col = 'light blue', main = "Unemployment")
hist(walmart_df$Store, col = 'light blue', main = "Store Size")
hist(walmart_df$Weekly_Sales, col = 'light blue', main = "Weekly_Sales")

```


```{r}
par(mfrow=c(1,1))
hist(walmart_df$Weekly_Sales, main = "Weekly Sales Distribution",xlab = "Weekly Sales")


```

```{r}
paste("Weekly_Sales has a skewness of", skewness(walmart_df$Weekly_Sales))
paste("Weekly_Sales has a kurtosis of", kurtosis(walmart_df$Weekly_Sales))

```

A skewness value greater than 1 or less than -1 indicates a highly skewed distribution. A value between 0.5 and 1 or -0.5 and -1 is moderately skewed. A value between -0.5 and 0.5 indicates that the distribution is fairly symmetrical.

The distribution of sales is clearly significantly skewed to the right. In this situation, the log function is typically used to account for the distributional skew. In the case of conventional approaches like regression analysis, the distribution of noise included in the data is frequently believed to be a symmetric distribution such as a normal distribution, which is why the distribution bias needs to be corrected.

If our initial data does not fit the bell curve, we may log convert it to make it as "normal" as possible, enhancing the validity of the statistical analysis results. Our weekly sales data is clustered for the low weekly sales because it is skewed to the right. 


```{r}

hist(log(walmart_df$Weekly_Sales), col = 'gray',
     main = "Weekly Sales log Transformed",
     xlab ='log(Weekly Sales)')

```
# Box plot distribution

```{r}
num_cols <- unlist(lapply(walmart_df, is.numeric))         # Identify numeric columns

data_numeric <- walmart_df[, num_cols]
data_numeric

boxplot(data_numeric$Store)
boxplot(data_numeric$Weekly_Sales)
boxplot(data_numeric$Holiday_Flag)
boxplot(data_numeric$Temperature)
boxplot(data_numeric$Fuel_Price)
boxplot(data_numeric$CPI)
boxplot(data_numeric$Unemployment)
```

# Correlation Matrix 

```{r}
corr = cor(walmart_df[, c(3:8)])
#view(corr)

corrplot(corr, method = "color", cl.pos = 'n', rect.col = "black",  tl.col = "indianred4", addCoef.col = "black", number.digits = 2, number.cex = 0.60, tl.cex = 0.7, cl.cex = 1, col = colorRampPalette(c("blue","white","red"))(100))
```
There is a moderate positive correlation between sales, fuel price and holidays, and negative correlation between sales, unemployment, CPI, and temperature. 

# Weekly sales distribution in differnt month

```{r}
ggplot(walmart_df, aes(x = Month,y = Weekly_Sales )) + 
  geom_col() +
  facet_wrap(~Year) + 
  ggtitle("Weekly Sales Distribution in Different Months")
```



```{r}
ggplot(walmart_df, aes(x=Store, y=Weekly_Sales, color=Holiday_Flag)) +
  geom_point() 
```




```{r}
ggplot(walmart_df, aes(x = Store,y = Weekly_Sales )) + 
  geom_col() +
  facet_wrap(~Year) + 
  ggtitle("Weekly Sales Distribution in Different Stores")
```

Which store has the maximum sales? 

```{r}
sales_store <- aggregate(Weekly_Sales ~ Store, data = walmart_df, sum)
sales_store <- arrange(sales_store, desc(Weekly_Sales))
sales_store
```

Store 20 has the maximum value/weekly sales among other stores. 

```{r}
sales_store$Store <- as.character(sales_store$Store)
sales_store$Store <- factor(sales_store$Store, levels=unique(sales_store$Store))

colnames(sales_store) <- c("Store","Weekly_Sales")

par(mfrow = c(1,1))

barplot(sales_store$Weekly_Sales,names=sales_store$Store,
        main="Weekly Sales by Store",
        xlab = "Store",
        ylab = "Weekly Sales",
        border="#69b3a2"
)

```


The highest sales are at store 20, with a total value of 301M. The revenue of shop 4 is 299M which considered as the second-largest store in terms of sales. Store 33 has the lowest sales which is around 37.17M. 

```{r}

#Creating Holidays Data dataframe
Holiday_date <- c("12-02-2010", "11-02-2011", "10-02-2012", "08-02-2013","10-09-2010", "09-09-2011", "07-09-2012", "06-09-2013","26-11-2010", "25-11-2011", "23-11-2012", "29- 11-2013","31-12-2010", "30-12-2011", "28-12-2012", "27-12-2013")

holidays <-c(rep("Super Bowl", 4), 
           rep("Labour Day", 4),
           rep("Thanksgiving", 4), 
           rep("Christmas", 4))
holidays

Holidays_Data <- data.frame(holidays,Holiday_date)
Holidays_Data

#merging both dataframes
walmart_df.2<-merge(walmart_df,Holidays_Data, by.x= "Date", by.y="Holiday_date", all.x = TRUE)
walmart_df.2

#Replacing null values in Event with No_Holiday 
walmart_df.2$holidays = as.character(walmart_df.2$holidays) 
walmart_df.2$holidays[is.na(walmart_df.2$holidays)]= "No_Holiday" 
head(walmart_df.2)

```

```{r}
Holiday_Sales<-aggregate(Weekly_Sales ~ holidays, data = walmart_df.2, mean)
Holiday_Sales
```

We have the most sales in the Thanksgiving and super bowl each year. 

```{r}
# plotting timeseries
walmart.ts  = ts(walmart_df$Weekly_Sales, frequency = 52,
                 start = c(2010,2), end = c(2012,10))
plot(walmart.ts, xlab="Time/Year", 
     ylab="Weekly Sales", bty='l')

```
The time series start in February 2010, and ends in October 2012, and has frequency of 52 weeks per year. 

```{r}

walmart.lm <- tslm(walmart.ts ~ trend + I(trend^2))
par(mfrow = c(1,1))
plot(walmart.ts, xlab="Time/Year", ylab="Weekly Sales", bty="l")
lines(walmart.lm$fitted, lwd=2)

```
It is helpful to divide a time series into a systematic component and a non-systematic part in order to select appropriate forecasting techniques, which the components are level, trend, seasonality and noise. From the figure above we can understand that the weekly sales for walmart organization has constant trend with additive seasonality. 

```{r}
walmart.ma = ma(walmart.ts, order = 48, centre = T) # As it is recorded yearly, there are 48 data points recorded per year, and we use a moving average window of 48.
plot(as.ts(walmart.ts))
lines(walmart.ma)
plot(as.ts(walmart.ma))
```


```{r}
decompose_walmart = decompose(walmart.ts, "additive")
 

plot(decompose_walmart)
```


# Modeling Time Series

Standard statistical models presuppose that observations are independent. This premise is false for time series. The reliance that only the past up to time t allows us to anticipate what will happen at time t+k is what we wish to model in time series. We refer to this type of dependence—where each observation is connected to itself at a prior time—as autocorrelation. If autocorrelation exists, the dependent variable must be appropriately delayed as predictive variables in the model.


## Autocorrelation

```{r}

par(mfrow = c(1,1))

acf = acf(walmart.ts, main='ACF Plot', lag.max=100)
pacf = pacf(walmart.ts, main='PACF Plot', lag.max=100)

#Augmented Dickey-Fuller(ADF) Test
print(adf.test(walmart.ts))

```
The p-value is obtained is less than significance level of 0.05 and the ADF statistic is -5.457, we reject the null hypothesis and consider the time series as a stationary. 


# Data partitioning

```{r}
walmart_df$Date[1]
walmart_df$Date[6435]
```


```{r}

# Partition the data into training and validation periods, so that years 2010 - October 2011 are the training period and the rest of the data is for validation

nValid <- 52
nTrain <- length(walmart.ts) - nValid
train.ts_df <- window(walmart.ts, start = c(2010, 2), end = c(2010, nTrain))
train.ts_df

valid.ts_df <- window(walmart.ts, start = c(2010, nTrain + 1), end = c(2012,10)) 
valid.ts_df

```

# Build ARIMA model


```{r}
# We apply this procedure to adjust the seasonality
walmart.ts %>%
  stl(s.window='periodic') %>% seasadj() -> eeadj.walmart
autoplot(eeadj.walmart)
```


```{r}
# Now we take a first difference of the weekly sales to remove trend and seasonality
eeadj.walmart %>%
  diff() %>% 
  ggtsdisplay(main="")

```
The AR(5) model is suggested by the PACF in the above figure. An ARIMA is thus a first candidate model (5,1,0)

```{r}
model1 <- Arima(eeadj.walmart, order=c(5,1,0))
model1

```

